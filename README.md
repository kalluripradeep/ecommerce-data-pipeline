# E-Commerce Data Pipeline

A production-ready data pipeline demonstrating scalable architecture patterns for batch and streaming data processing.

## Overview

This project implements a three-zone data pipeline architecture (Raw → Curated → Refined) for processing e-commerce transaction data. It demonstrates best practices for data quality, orchestration, and cloud-native design.

## Architecture

```
┌─────────────┐      ┌──────────────┐      ┌─────────────┐
│  Raw Zone   │ ───> │ Curated Zone │ ───> │Refined Zone │
│  (S3/ADLS)  │      │   (PySpark)  │      │ (Snowflake) │
└─────────────┘      └──────────────┘      └─────────────┘
       │                     │                      │
       │                     │                      │
    Landing              Processing             Analytics
    Storage            & Validation            & BI Layer
```

## Features

### ✅ Data Ingestion
- Batch file ingestion from S3
- Configurable file format support (CSV, JSON, Parquet)
- Incremental loading with date partitioning
- Schema validation on ingestion

### ✅ Data Quality
- Comprehensive validation using Great Expectations
- Null checks, range validation, referential integrity
- Data profiling and anomaly detection
- Quality metrics tracking

### ✅ Data Transformation
- PySpark-based transformations
- Deduplication logic
- Type casting and standardization
- Business rule implementation

### ✅ Orchestration
- Airflow DAGs for workflow management
- Task dependencies and error handling
- Retry logic and alerting
- Idempotent pipeline design

### ✅ Monitoring
- Pipeline execution metrics
- Data quality dashboards
- Freshness monitoring
- Volume trend analysis

## Tech Stack

- **Processing:** PySpark, Python 3.9+
- **Orchestration:** Apache Airflow
- **Storage:** AWS S3 (can be adapted for Azure/GCP)
- **Data Quality:** Great Expectations
- **Warehouse:** Snowflake (adaptable to Redshift/BigQuery)
- **Infrastructure:** Docker, Terraform (optional)

## Project Structure

```
ecommerce-data-pipeline/
├── README.md
├── requirements.txt
├── config/
│   ├── pipeline_config.yaml
│   └── quality_checks.yaml
├── src/
│   ├── ingestion/
│   │   ├── batch_ingestion.py
│   │   └── schema_validator.py
│   ├── transformation/
│   │   ├── transactions_processor.py
│   │   ├── customers_processor.py
│   │   └── deduplication.py
│   ├── quality/
│   │   ├── expectations.py
│   │   └── monitors.py
│   └── utils/
│       ├── spark_utils.py
│       └── logging_utils.py
├── airflow/
│   └── dags/
│       ├── daily_transaction_pipeline.py
│       └── hourly_streaming_pipeline.py
├── tests/
│   ├── test_ingestion.py
│   ├── test_transformation.py
│   └── test_quality.py
└── docs/
    ├── architecture.md
    ├── setup.md
    └── troubleshooting.md
```

## Key Components

### 1. Ingestion Layer

Handles data loading from source systems into the raw zone.

```python
# src/ingestion/batch_ingestion.py
class BatchIngestion:
    def ingest_transactions(self, date, source_path, raw_path):
        """
        Ingest transaction files from source to raw zone
        - Validates file exists
        - Checks schema compatibility
        - Loads to partitioned raw storage
        """
        pass
```

### 2. Transformation Layer

Processes data from raw to curated zone with quality checks.

```python
# src/transformation/transactions_processor.py
class TransactionProcessor:
    def process(self, raw_df):
        """
        Process raw transactions:
        - Remove duplicates
        - Validate data types
        - Apply business rules
        - Flag invalid records
        """
        pass
```

### 3. Quality Layer

Implements comprehensive data quality checks.

```python
# src/quality/expectations.py
class QualityChecker:
    def validate_transactions(self, df):
        """
        Run quality checks:
        - Not null validations
        - Range checks
        - Business rule validations
        - Volume checks
        """
        pass
```

### 4. Orchestration

Airflow DAGs manage the entire workflow.

```python
# airflow/dags/daily_transaction_pipeline.py
dag = DAG(
    'daily_transaction_pipeline',
    schedule_interval='@daily',
    default_args={'retries': 2}
)

ingest >> validate >> transform >> load >> quality_check
```

## Setup Instructions

### Prerequisites

- Python 3.9+
- Apache Spark 3.x
- Apache Airflow 2.x
- AWS Account (or Azure/GCP equivalent)
- Docker (optional, for local development)

### Installation

1. Clone the repository
```bash
git clone https://github.com/kalluripradeep/ecommerce-data-pipeline.git
cd ecommerce-data-pipeline
```

2. Install dependencies
```bash
pip install -r requirements.txt
```

3. Configure environment
```bash
cp config/pipeline_config.example.yaml config/pipeline_config.yaml
# Edit config file with your settings
```

4. Run tests
```bash
pytest tests/
```

## Configuration

Edit `config/pipeline_config.yaml`:

```yaml
source:
  s3_bucket: "your-raw-bucket"
  file_format: "parquet"
  
processing:
  spark_master: "local[*]"
  executor_memory: "4g"
  
quality:
  null_threshold: 0.05
  volume_check: true
  anomaly_detection: true
  
alerting:
  slack_webhook: "your-webhook-url"
  email: "your-email@company.com"
```

## Usage

### Run Full Pipeline

```bash
python src/main.py --date 2024-11-20
```

### Run Individual Components

```bash
# Ingestion only
python src/ingestion/batch_ingestion.py --date 2024-11-20

# Transformation only
python src/transformation/transactions_processor.py --date 2024-11-20

# Quality checks only
python src/quality/expectations.py --date 2024-11-20
```

### Airflow Integration

```bash
# Copy DAGs to Airflow
cp airflow/dags/* $AIRFLOW_HOME/dags/

# Trigger DAG
airflow dags trigger daily_transaction_pipeline
```

## Data Quality Checks

The pipeline implements 20+ quality checks including:

- **Completeness:** Null checks, row count validation
- **Accuracy:** Range validation, format checks
- **Consistency:** Referential integrity, cross-table validation
- **Timeliness:** Freshness monitoring, SLA tracking

Quality metrics are logged and can be visualized in dashboards.

## Performance

Benchmarks on sample dataset (10M transactions):

- **Ingestion:** ~2 minutes
- **Transformation:** ~5 minutes
- **Quality Checks:** ~1 minute
- **Total End-to-End:** ~8 minutes

Scales linearly with Spark cluster size.

## Monitoring

The pipeline includes:

- Execution time tracking
- Data volume metrics
- Quality score trends
- Error rate monitoring
- Cost tracking (for cloud resources)

Metrics can be exported to Prometheus, Grafana, or your monitoring stack.

## Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test suite
pytest tests/test_transformation.py
```

## Troubleshooting

Common issues and solutions documented in [docs/troubleshooting.md](docs/troubleshooting.md)

## Contributing

This is a demonstration project for portfolio purposes. Feedback and suggestions welcome!

## Design Decisions

Key architectural choices and trade-offs:

1. **Three-zone architecture** for clear separation of concerns
2. **Idempotent design** allows safe pipeline reruns
3. **Quality-first** approach catches issues early
4. **Cloud-native** leverages managed services where appropriate
5. **Modular design** allows component reuse and testing

## Future Enhancements

- [ ] Real-time streaming support with Kafka
- [ ] ML feature store integration
- [ ] CDC (Change Data Capture) implementation
- [ ] Multi-cloud support
- [ ] Advanced anomaly detection with ML

## License

MIT License - free to use for learning and portfolio purposes

## Contact

Pradeep Kalluri
- LinkedIn: [linkedin.com/in/pradeepkalluri](https://linkedin.com/in/pradeepkalluri)
- Medium: [@pradeepkalluri](https://medium.com/@pradeepkalluri)

---

**Note:** This is a demonstration project showcasing production-ready patterns and best practices. Sample data and configurations should be adapted for your specific use case.
